{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31xzMxH89wic",
        "outputId": "27f8b915-252f-42d1-e273-d5f187bef3ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvESLDtVd_lR"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import Input\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Bidirectional, Dense, BatchNormalization, Conv1D, Conv2D, MaxPooling1D, Dropout, Flatten, Reshape\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eA1h6ka9uEv"
      },
      "source": [
        "def load_all_data_LSTM(inPath, SEQUENCE_N):\n",
        "  # train_features, train_labels = np.array([[0]*feature_n]), np.array([[0]*label_n])\n",
        "  train_features, train_labels = [], []\n",
        "\n",
        "  file_paths = [os.path.join(inPath, f) for f in next(os.walk(inPath))[2] if f.endswith('.npy')]\n",
        "  for fileIdx, path in enumerate(file_paths):    \n",
        "    file_data = np.load(path, allow_pickle=True)\n",
        "\n",
        "    # preprocess_data_LSTM\n",
        "    seqImages = np.array([subsubdata[0].numpy()[0] for subdata in file_data for subsubdata in subdata], dtype=np.float32)\n",
        "    seqLabels = np.array([subsubdata[1] for subdata in file_data for subsubdata in subdata], dtype=np.float32)\n",
        " \n",
        "    # append splited sequence data\n",
        "    sub_train_features, sub_train_labels = [], []\n",
        "    # sub_train_features, sub_train_labels = np.array([[0]*feature_n]), np.array([[0]*label_n])\n",
        "    for i in range(0, len(seqImages), SEQUENCE_N):\n",
        "      sub_train_features.append(seqImages[i:i+SEQUENCE_N])\n",
        "      sub_train_labels.append(seqLabels[i+SEQUENCE_N-1])\n",
        "    train_features.extend( np.array(sub_train_features, dtype=np.float32) )\n",
        "    train_labels.extend( np.array(sub_train_labels, dtype=np.float32) )\n",
        "\n",
        "    #   sub_train_features = np.append(sub_train_features, seqImages[i:i+SEQUENCE_N], axis=0)\n",
        "    #   sub_train_labels = np.append(sub_train_labels, seqLabels[i:i+SEQUENCE_N], axis=0)\n",
        "    # train_features = np.append(train_features, sub_train_features, axis=0)\n",
        "    # train_labels = np.append(train_labels, sub_train_labels, axis=0)\n",
        "\n",
        "  # train_features, train_labels = np.array(train_features, dtype=np.float32), np.array(train_labels, dtype=np.float32)\n",
        "  train_features, train_labels = np.array(train_features), np.array(train_labels)\n",
        "  print(f'features shape: {train_features.shape}\\nlabels shape: {train_labels.shape}')\n",
        "  # return np.expand_dims(train_features[1:], 0), np.expand_dims(train_labels[1:], 0)\n",
        "  return train_features, train_labels\n",
        "\n",
        "\n",
        "\n",
        "# Generate class weights for imbalanced data\n",
        "from collections import Counter\n",
        "def getWeights(train_labels):\n",
        "  labelLen = train_labels.shape[0]\n",
        "\n",
        "  # onehotLabels = [np.where(onehotLabel==1)[0][0] for onehotLabel in train_labels]\n",
        "  # labelWeightDict = {k: 1-(v/labelLen) for k,v in dict(Counter(onehotLabels)).items()}\n",
        "  labelWeightDict  = {0: 0.5300869565217392, 1: 0.957391304347826, 2: 0.9426086956521739, \n",
        "                    3: 0.9464347826086956, 4: 0.9579130434782609, 5: 0.965391304347826, \n",
        "                    6: 0.9949565217391304, 7: 0.9928695652173913, 8: 0.7123478260869565}\n",
        "\n",
        "  labelWeights = np.asarray([labelWeightDict[np.where(onehotLabel==1)[0][0]] for onehotLabel in train_labels])\n",
        "  return labelWeights\n",
        "\n",
        "input_n, feature_n, label_n = 500, 1536, 9\n",
        "def get_LSTM(SEQUENCE_N):\n",
        "  # LSTM input hidden layer\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(feature_n, \n",
        "                #  input_shape = (timesteps, n_features),\n",
        "                input_shape = (SEQUENCE_N, feature_n), \n",
        "                activation='relu',\n",
        "                return_sequences=True))\n",
        "  \n",
        "  # normalization layer\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dropout(0.3))\n",
        "  \n",
        "  # LSTM hidden layer\n",
        "  model.add(LSTM(units=feature_n//2, activation='relu', return_sequences = True))\n",
        "  model.add(Dropout(0.35))\n",
        "  \n",
        "  # LSTM hidden layer\n",
        "  model.add(LSTM(units=feature_n//5, activation='relu', return_sequences = True))\n",
        "  model.add(Dropout(0.4))\n",
        "  \n",
        "  # LSTM hidden layer\n",
        "  model.add(LSTM(units=feature_n//10, activation='relu', return_sequences = False))\n",
        "  # model.add(Dropout(0.45))\n",
        "  \n",
        "  # model.add(MaxPooling1D(pool_size=3, padding='same', data_format='channels_last'))\n",
        "  # model.add(Conv1D(filters = feature_n//4,  # dimensionality of the output space\n",
        "  #                  kernel_size = 3, # length of the convolution window\n",
        "  #                  data_format = 'channels_last',\n",
        "  #                  padding = 'valid', \n",
        "  #                  activation = 'relu'))\n",
        "\n",
        "  #  Hidden layer\n",
        "  # model.add(Flatten())\n",
        "  # model.add(Dense(SEQUENCE_N*label_n, activation='relu'))\n",
        "\n",
        "  # Softmax output layer \n",
        "  # model.add(Dense(feature_n, activation='softmax'))\n",
        "  model.add(Dense(label_n, activation='softmax'))\n",
        "\n",
        "\n",
        "  # model.add(Reshape((1, SEQUENCE_N, label_n)))\n",
        "\n",
        "  # Cross Entropy objective function\n",
        "  model.compile(loss = 'categorical_crossentropy',\n",
        "                optimizer = tf.keras.optimizers.Adam(clipvalue=5, learning_rate=0.0001),\n",
        "                # optimizer = 'adam', # ADAM optimization algorithm for speed\n",
        "                # run_eagerly=True,\n",
        "                metrics = ['accuracy']) \n",
        "\n",
        "  print(f'input shape: {model.input_shape}\\noutput shape:{model.output_shape}')\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "  \n",
        "# def train(model, train_X, train_Y_onehot, epoch, batch=None):\n",
        "def train(modelOutPath, tensorboard, model, train_X, train_Y_onehot, epoch, batch=1, valid_X=None, valid_Y=None):\n",
        "  checkpointer = ModelCheckpoint(\n",
        "                                # filepath=  os.path.join(outPath, f\"model.{epoch:02d}-{val_loss:.2f}.hdf5\"), \n",
        "                                filepath=modelOutPath,\n",
        "                                verbose = 1, \n",
        "                                period=1, # number of epochs between checkpoints\n",
        "                                # save_weights_only=True,\n",
        "                                save_best_only = True,\n",
        "                                monitor = 'loss', mode = 'min')\n",
        "\n",
        "  earlystop = EarlyStopping(monitor='loss',\n",
        "                            patience=10, \n",
        "                            verbose=1, \n",
        "                            mode=\"auto\",\n",
        "                            restore_best_weights=True)\n",
        "\n",
        "  train_history = model.fit(train_X, train_Y_onehot,\n",
        "                            batch_size = batch, \n",
        "                            epochs = epoch, \n",
        "                            shuffle = True,\n",
        "                            sample_weight = getWeights(train_labels),\n",
        "                            # validation_split = 0.2,\n",
        "                            validation_data = (valid_X, valid_Y), \n",
        "                            callbacks = [checkpointer, earlystop, tensorboard],\n",
        "                            )\n",
        "  model.save(modelOutPath)\n",
        "  print(f'Save model at {modelOutPath}')\n",
        "  return train_history\n",
        "\n",
        "def get_tensorboard(model, log_dir='CSCI527Project/Models/LSTM/tb_logs'):\n",
        "  tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir,\n",
        "      histogram_freq=0,\n",
        "      # batch_size=batch,\n",
        "      # update_freq='batch',\n",
        "      write_graph=True,\n",
        "      write_grads=True\n",
        "  )\n",
        "  tensorboard.set_model(model)\n",
        "\n",
        "  return tensorboard\n",
        "\n",
        "def drawResult(history, epoch):\n",
        "  result = {index:[] for index in history.keys()}\n",
        "  for index, val in result.items():\n",
        "    plt.plot(range(epoch), val, label=index)\n",
        "\n",
        "  plt.xlabel('epoch'); plt.ylabel('error')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12ubF-AnsEzf"
      },
      "source": [
        "inPath = 'CSCI527Project/Object detection features/training data'\n",
        "inPath_valid =  'CSCI527Project/Object detection features/validation data'\n",
        "\n",
        "epoch = 20\n",
        "\n",
        "for SEQUENCE_N in [50]:\n",
        "# for SEQUENCE_N in [50, 10]:\n",
        "  modelOutPath = f'CSCI527Project/Models/LSTM/test_model_LSTM_epochs_{epoch}_seq_{SEQUENCE_N}.h5'\n",
        "  log_dir = f'CSCI527Project/Models/LSTM/tb_logs_epochs_{epoch}_seq_{SEQUENCE_N}'\n",
        "\n",
        "  # load LSTM data in sequences\n",
        "  train_features, train_labels = load_all_data_LSTM(inPath, SEQUENCE_N)\n",
        "  valid_features, valid_labels = load_all_data_LSTM(inPath_valid, SEQUENCE_N)\n",
        "  print(train_features.shape, train_labels.shape)\n",
        "\n",
        "  # train LSTM model\n",
        "  model = get_LSTM(SEQUENCE_N)\n",
        "  \n",
        "  tensorboard = get_tensorboard(model, log_dir)\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir 'CSCI527Project/Models/LSTM/tb_logs_epochs_10_seq_50'\n",
        "\n",
        "  # history = train(model, train_features, train_labels, epoch)\n",
        "\n",
        "  history = train(modelOutPath, tensorboard, \n",
        "                  model, train_features, train_labels, epoch, \n",
        "                  valid_X=valid_features, valid_Y=valid_labels, \n",
        "                  batch=16)\n",
        "  # drawResult(history, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RzNt-IKhuY5"
      },
      "source": [
        "epoch = 20\n",
        "SEQUENCE_N = 50\n",
        "modelOutPath = f'CSCI527Project/Models/LSTM/test_model_LSTM_epochs_{epoch}_seq_{SEQUENCE_N}.h5'\n",
        "log_dir = f'CSCI527Project/Models/LSTM/tb_logs_epochs_{epoch}_seq_{SEQUENCE_N}'\n",
        "\n",
        "model = get_LSTM(SEQUENCE_N)\n",
        "tensorboard = get_tensorboard(model, log_dir)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'CSCI527Project/Models/LSTM/tb_logs_epochs_20_seq_50'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rydjvDczv2uv"
      },
      "source": [
        "# !pip install -U tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "# Launch tensorboard in url\n",
        "def lunchTensorboard(log_dir):\n",
        "  tensorboard_launch_cmd = f\"tensorboard dev upload --logdir '{log_dir}'\"; # added missing \" at the end    \n",
        "  !$tensorboard_launch_cmd\n",
        "\n",
        "lunchTensorboard(log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}